# 🐥 밑바닥부터 시작하는 딥러닝3 실습



## 🔑 Keywords



### ➡️ Variable

### ➡️ Fucntion

### ➡️ Numercial Differentiation

- 미세한 차리를 이용하여 함수의 변화량을 구하는 방법을 '수치 미분(numercial differentiation)'이라고 한다.

- 아무리 작은 값을 사용하여도 오차가 발생 할 수 있다.

  → x-h, x+h 에서의 기울기를 구하는 방법 **'중앙 차분(centered difference)'**

  → x, x+h 지점에서의 기울기를 구하는 방법 **'전진 차분(forward difference)'**

  중앙 차분이 오차가 더 적다!

**💦 수치 미분의 문제점**

1. 자릿수 누락 → 차이를 구하는 계산에서 주로 크기가 비슷한 값들을 다루므로 계산 결과에서 유효 자릿수가 줄어들 수 있음.

> **ex)** 1.234... - 1.233... = 0.001434...
>
> 유효자릿수가 4일 때 1.234 - 1.233 = 0.001 ***(유효 자릿수가 1로 줄어듦)***

2. 계산량이 많음 → 신경망에서는 매개변수가 수백만 개 이상 사용하는 경우도 많음.

✨ 위의 한계를 극복한 **역전파**, 역전파를 정확하게 구현했는지 확인하기 위해 수치 미분의 결과를 이용

### ➡️ Backpropagation

**Chain Rule (연쇄 법칙)**

- 합성함수의 미분은 구성 함수 각각을 미분한 후 곱한 것과 같음.
  - 전파되는 값은 최종 결과인 y값의 미분값들

**Loss Function (손실함수)**

- 대개 머신러닝 모델은 대량의 매개변수를 입력받아서 마지막에 손실함수를 거쳐 출력을 내는 형태로 진행

  - (많은 경우) 단일 스칼라값

- 손실 함수의 각 매개변수에 대한 미분을 계산해야함 → 출력에서 입력 방향으로 한번만 전파하면 모든 매개변수에 대한 미분을 계산할 수 있음.

**Gradient (기울기)**

벡터나 행렬 등 다변수에 대한 미분

> **Define-by-Run (동적 계산 그래프)**
>
> 데이터를 흘려보냄으로써(Run함으로써) 연결이 규정된다(Define 된다)
>
> → 딥러닝에서 수행하는 계산들을 계산 시점에 '연결' 하는 방식 (체이너와 파이토치의 방식)
>
> → 분기가 있는 계산 그래프에의 응용을 위한 웬거트 리스트( Wengert List)

### ➡️ Python Unit Test

```python
python -m unittest test.py

# 특정 디렉토리 아래 모두를 실행하고 싶으면
python -m unittest discover <dir_name>
```

**✨ 컴퓨터 프로그램에서 미분을 계산하는 방법**

- Numercial Differentiation (수치 미분)

  변수에 미세한 차이를 주어 일반적인 계산(순전파) 2회 시행 후 출력의 차이로부터 근사적으로 미분을 계산, 다량의 변수를 사용하느 함수를 다룰 때의 계산 비용이 높음.

- Symbolic Differentiation (기호 미분)

  공식으로 계산, 최적화를 고혀하지 않고 구현하면 수식이 곧바로 거대해질 우려가 있음.

- Automatic Differentiation (자동 미분)

  연쇄 법칙의 사용

  - forward mode
  - reverse mode == 역전파

### ➡️ Partial Derivative

Partial Derivative(편미분)은 입력 변수가 여러개인 다변수 함수에서 하나의 입력 변수에만 주목하여(다른 변수는 상수로 취급) 미분하는 것을 뜻함.
$$
{\partial y \over \partial x_0} = 1, {\partial y \over \partial x_1} =1
$$

### ➡️ Topology

그래프의 연결된 형태를 Topology(위상)이라고 한다. 다양한 위상의 계산 그래프 미분에 대응할 수 있도록 구조 변경!



복잡한 계산 그래프에서 함수의 우선순위를 정할 수 있도록 **Topology Sort(위상정렬)** 활용

→ 함수의 세대(generation)를 기록하기



### ➡️ Memory

- 파이썬은 필요 없어진 객체를 인터프리터가 메모리에서 자동으로 삭제한다.

  → 메모리 관리를 의식할 일이 크게 줄어든다.

- Memory Leak(메모리 누수)이나 Out of Memory(메모리 부족) 문제에 항상 대비해야 한다.



**👉 CPython의 메모리 관리 방식**

1. Reference(참조) 수를 세는 방식 → 참조 카운트
2. Generation(세대)을 기준으로 쓸모없어진 객체를 회수 → Garbage Collection(가비지 컬렉션)



**1️⃣ 참조 카운트 방식**

모든 객체는 참조 카운트가 0인 상태로 생성, ***다른 객체가 참조할 때마다 1씩 증가***

반대로, ***객체에 대한 참조가 끊길 때 마다 1만큼 감소***, 0이 되면 파이썬 인터프리터가 회수

- 참조 카운트가 증가하는 경우

  - 대입 연산자를 사용할 때
  - 함수에 인수로 전달할 때
  - 컨테이너 타입 객체(리스트, 튜플, 클래스 등)에 추가할 때

```python
class obj:
  pass

def f(x):
  print(x)

a = obj() # obj()에 의해 생성된 개체를 a 변수에 대입: 참조 카운트 1
f(a) # 함수에 전달: a가 인수로 전달되기 때문에 함수 안에서는 참조 카운트 2
# 함수 완료: 함수 범위를 벗어나오면 참조 카운트 1
a = None # 대입 해제: 참조를 끊으면(아무도 참조하지 않는 상태) 참조 카운트 0
```
```python
a = obj()
b = obj()
c = obj()
# a - 1, 변수 대입
# b - 1, 변수 대입
# c - 1, 변수 대입

a.b = b
b.c = c
# a - 1
# b - 2, a가 b를 참조
# c - 2, b가 c를 참조

a = b = c = None
# a - 0, 참조 끊음
# b - 1, 참조 끊음
# c - 1, 참조 끊음
```
>a의 참조카운트 0 → a 즉시 삭제 → a 삭제의 여파로 b의 참조 카운트가 1에서 0으로 감소 → b 삭제 → b삭제의 여파로 c의 참조 카운트가 1에서 0으로 감소 → c 삭제



**2️⃣ 순환 카운트 방식**

```python
a = obj()
b = obj()
c = obj()
# a - 1, 변수 대입
# b - 1, 변수 대입
# c - 1, 변수 대입

# a,b,c 세개의 객체가 원 모양을 이루며 서로가 서로를 참조
a.b = b
b.c = c
c.a = a
# b - 2, a가 b를 참조
# c - 2, b가 c를 참조
# a - 2, c가 a를 참조

a = b = c = None
# a - 1, 참조 끊음
# b - 1, 참조 끊음
# c - 1, 참조 끊음
```

> → `a = b = c = None` 을 실행하는 것으로 순환 참조의 참조 카운트가 0이 되지 않는다(메모리에서 삭제되지 않음), 하지만 셋 다 접근이 불가능 (불필요한 객체)
>
> 이에 대한 대안이 **"Generational Garbage Collector(세대별 가비지 컬렉션)"**
>
> - 메모리가 부족해지는 시점에서 파이썬 인터프리터에 의해 자동 호출
> - 명시적 호출 가능 (gc 모듈 임포트 후 `gc.collect()`)

